# N-gram-Language-Model
A fun little project I made while bored one afternoon. Generates badly-worded philosophical text. 

Works by starting with a string of tokens (made up of n words or substrings) and continuing the text by adding tokens which usually come after this combination of tokens in the training data. This is a very simplistic approach to text generation, and often makes no sense. 
